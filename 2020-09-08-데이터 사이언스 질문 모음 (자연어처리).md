---
layout: post
title:  "데이터 사이언스 질문 모음 (자연어처리)"
date:   2020-09-08 09:02:00 +0900
categories: [deeplearning]
use_math: true
---

## 데이터 사이언스 질문 모음 (자연어처리)

이 포스팅은 유명한 <[https://zzsza.github.io/data/2018/02/17/datascience-interivew-questions/#%EC%9E%90%EC%97%B0%EC%96%B4-%EC%B2%98%EB%A6%AC](https://zzsza.github.io/data/2018/02/17/datascience-interivew-questions/#자연어-처리)> 에서 정리해둔 질문 모음에 대한 나의 답변을 최대한 적어두었다. (정확하지 않을 수도 있고 계속 수정해 나아가며 정리할 예정)

### 자연어 처리

* One-hot 인코딩에 대한 설명을 해주세요
  * <https://jsstar522.github.io/deeplearning/2020/09/02/One-hot-encoding.html>
* POS 태깅은 무엇인가요? 가장 간단하게 POS tagger을 만드는 방법은 무엇일까요?
  * POS 태깅은 문장의 품사를 매기는 방식입니다. 품사란 명사, 대명사, 수사 등을 얘기합니다. 언어마다 다른 품사의 종류를 가지고 있습니다.
  * POS 태깅은 RNN으로 구현할 수 있습니다. RNN은 문장 단어가 순서대로 input으로 들어갈 때마다 각 단어의 output을 내놓게 되어 있습니다. 그 output의 target을 품사로 정하면 되고 품사의 종류는 여러가지이기 때문에 softmax 함수를 activation function으로 사용하면 됩니다. 
  * 가장 간단하게 만드는 방법은 형태소 분석기를 사용하는 방법입니다. 유명한 `KoNLPy (nltk)` 라이브러리가 있습니다. 이 안에 분석기의 종류에는 꼬꼬마, 코모란, 트위터가 있습니다.
* 문장에서 "Apple"이란 단어가 과일인지 회사인지 식별하는 모델을 어떻게 훈련시킬 수 있을까요?
  * Apple이라는 단어 앞 뒤에 오는 단어를 분석하면 됩니다. '먹다', '깎다' 등의 단어가 오면 과일이고 '스마트폰', '주식'과 같은 단어가 오면 회사입니다. Apple 이라는 단어를 찾아서 앞 뒤에 있는 단어와 함께 학습시키면 과일인지 회사인지 알 수 있는 모델을 훈련시킬 수 있습니다.
* 뉴스 기사에 인용된 텍스트의 모든 항목을 어떻게 찾을 수 있을까요?
* 음성 인식 시스템에서 생성된 텍스트를 자동으로 수정하는 시스템을 어떻게 구축할까요?
  * 문장이 제대로 되었는지 파악하려면 토큰화를 해봐야하는데 토큰화를 하려면 먼저 띄어쓰기가 잘 되어 있어야 합니다. 띄어쓰기를 수정하는 모델이 필요하고 띄어쓰기 수정이 이루어지면 토큰화를 해야합니다. 토큰화를 한 뒤 문장이 자연스러운지 평가해야합니다. 예를 들어 "친구랑 코피를 마셨어"라는 단어는 부자연스럽기 때문에 단어를 바꿔줘야한다는 판단을 내려야 합니다. "코피"와 비슷한 발음이 나는 단어들 중 문맥에 가장 잘 어울리는 단어를 대입시킵니다. 이는 검색창에서 오타를 냈을 때 "~을 검색했나요?"라고 나오는 것 기능과 비슷할 것 같습니다.
* 잠재론적, 의미론적 색인은 무엇이고 어떻게 적용할 수 있을까요?
  * 잠재의미분석이란 `단어-문서 행렬` 이나 `단어-문맥 행렬` 등 입력 데이터에 특이값 분해를 수행하여 데이터의 차원수를 줄여 계산 효율성을 키우고 숨어있는 의미 (동의어)를 이끌어 내기 위한 방법론입니다. 
* 영어 텍스트를 다른 언어로 번역할 시스템을 어떻게 구축해야 할까요?
  * 단어로 구성된 문장 $\to$ 모든 단어 토큰화 및 임베딩 (인코더) $\to$ Network (RNN 기반) $\to$ 디코더 $\to$ 정답 (번역된 말)
  * Attention 기법은 번역문의 특정 단어를 예측할 때 원문에 대응하는 단어의 가중치 정보를 학습하여 활용하는 방식입니다.
  * https://tech.kakaoenterprise.com/22
* 뉴스 기사를 주제별로 자동 분류하는 시스템을 어떻게 구축할까요?
  * 리뷰 데이터 분류 (긍정, 부정)보다 더 넓은 네트워크를 구축해야합니다. 왜냐하면 카테고리가 굉장히 많을 수 있기 때문입니다. 네트워크가 좁으면 각 카테고리에 대한 정보가 없어질 가능성이 있습니다. 학습이 시작되기 전, 각각의 뉴스 기사데이터는 형태소 분류와 POS taggin이 이루어져야 합니다. 그리고 모든 단어들에 대해 토큰화를 시켜주고 벡터화 해줍니다. (임베딩) 입력값은 뉴스 기사 데이터가 될 것이고, 출력값은 카테고리가 될 것이며 카테고리 개수에 맞는 크기의 벡터를 출력해야 합니다. 그렇기 때문에 마지막 activation function은 softmax을 사용하면 되고, loss function은 categorical cross_entropy을 사용하면 됩니다.
  * Softmax 함수는 출력값의 class 분류 (category 분류)를 위해 정규화를 해주는 함수입니다. 벡터 형태로 바꿔주며 모든 요소의 합은 항상 1입니다. 어느 한 벡터의 요소가 가장 크다면 그 input 값은 가장 큰 값을 가지고 있는 요소가 의미하는 카테고리라고 할 수 있습니다.
* Stop Words은 무엇일까요? 이것을 왜 제거해야 하나요?
  * Stop words은 `불용어` 라고 불리는데, 텍스트 데이터를 분석하는 데에 있어서 큰 역할을 하지 않는 단어를 의미합니다. 예를들어, I, you, me, my와 같은 단어들이고 이는 기본적으로 `nltk` 에 정의가 되어 있지만 사용자가 직접 정의해줄 수도 있습니다. 
  * 불용어가 많이 포함되어 있으면 첫번째로 처리시간이 오래걸릴 수 있습니다. 또한 자주 나오는 단어이지만 쓸모없는 단어를 의미하기 때문에 반대로 생각하여 학습 과정에서 이 단어들이 포함되어 있으면 자주 나오기 때문에 **쓸모없지만 중요하게 인식될 수 있습니다.**
  * 예를들어 워드클라우드를 만들 때 불용어 제거를 해주지 않는다면 대부분 the, I, you와 같은 단어들만 그려져 있을 것입니다.
* 영화 리뷰가 긍정적인지 부정적인지 예측하기 위해 모델을 어떻게 설계하시겠나요?
  * 먼저 문장을 토큰화 해야합니다. 그 이후에 RNN을 사용하면 텍스트가 긍정인지 부정인지 예측하는 모델을 만들 수 있습니다. RNN 구조에서 단어가 순서대로 네트워크를 통과하면 hidden state은 앞에 들어갔던 단어들의 정보를 함축하고 있습니다. 그리고 모든 단어가 들어간 뒤의 hidden state은 그 문장에 있는 모든 단어의 어떤 특성을 함축하고 있다는 뜻입니다. 그 특성을 `긍정`과 `부정` 으로 정하고 target으로 정해주면 됩니다. 긍정과 부정이기 때문에 binary cross entropy을 loss function으로 사용하면 됩니다.
* TF-IDF 점수는 무엇이며 어떤 경우 유용한가요?
  * `Term Frequency-Inverse Document Frequency` 의 줄인말입니다. 
  * 먼저 `TF` 은 특정 단어가 문서내에 얼마나 자주 등장하는지 나타내는 값입니다. `IDF` 은 `DF(document frequency)` 의 역수입니다. `TF-IDF` 은 TF와 IDF을 곱한 값으로 **값이 높을 수록 다른 문서에는 많지 않고 해당 문서에 자주 등장하는 단어를 의미합니다.** 
  * 간단하게 구현할 수 있습니다. 문서 내에서 해당 단어의 개수를 구하고, 이 단어가 다른 문서에도 있는 경우도 체크하면서 값을 구해둡니다. 이 둘을 계산하면 됩니다.
  * word2vec 에서 가중치를 매기는데 쓰입니다. 예를들어 `TF-IDF` 가 높은 단어가 높은 가중치로 토큰화 되고, 이 상태로 문서 분석에 활용된다면 그 단어가 문서의 키워드가 될 가능성이 높습니다.
* 한국어에서 많이 사용되는 사전은 무엇인가요?
* Finite Automat (유한 오토마타)에 대해서 설명해주세요.
  * 
* Regular grammar은 무엇인가요? Regular expression과 무슨 차이가 있나요?
  * 유한 오토마타 이외에 정규언어를 표현하는 방식에는 Regular grammar와 Regular expression이 있습니다.
  * 
* RNN에 대해 설명해주세요.
  * <https://jsstar522.github.io/machinelearning/2020/03/01/RNN(Recurrent-Neural-Network)에-대해서.html>
  * Sequence data을 학습시킬 때 유용한 모델입니다. Sequence data란 앞뒤 순서가 있는 시계열 데이터나 텍스트 데이터를 의미합니다. RNN은 순서상 먼저 들어온 데이터가 나중에 들어올 데이터에 영향을 주는 구조입니다. 먼저 들어온 데이터가 Input으로 들어간 뒤 나온 Ouput이 그 다음 데이터와 함께 Input으로 다시 들어갑니다. 그래서 Recurrent Nueral Network라는 이름이 붙었습니다. 특정 위치의 `Output`은 그 `다음의 Input`과 함께 그대로 들어가지 않고 **가중치가 곱해진 뒤에** 다음의 Input과 **더해져서** 들어갑니다. 이렇게 더해지는 값을 hidden state라고 합니다. 거꾸로 말해보면 hidden state은 앞의 순서의 데이터의 정보를 반영하고 있습니다. 
* LSTM은 왜 유용한가요?
* Translate 과정 Flow에 대해 설명해주세요
  * 가장 쉬운 방법으로는 단어마다 1대1로 번역단어를 대응시키면 되지만 이 방법은 문맥이나 언어마다 다른 문법을 고려하지 않을 수 있기 때문에 기계번역 기술을 사용하는 것이 유리합니다. 가장 대표적으로 `seq2seq 모델`이 있습니다. 
  * FLOW
    * 번역할 언어로 작성된 소스 문장을 토큰화 및 임베딩 하고 RNN 모델의 인풋으로 넣습니다.
    * 소스 문장의 가장 마지막 단어를 인풋으로 넣고 구한 RNN의 state을 타겟 문장(번역된 문장)의 첫번째 단어의 예측을 시작할 때 초기 state로 지정합니다.
    * `<s>` 은 문장의 시작을 의미하는 키워드로 번역할 언어로 작성된 타겟 문장의 첫번째 단어로 사용합니다.
    * `<s>` 을 넣고 RNN이 다음에 올 수 있는 타겟 단어 후보군을 softmax 행렬 형태로 예측하면 다음에 올 가장 그럴듯한 단어 (softmax 행렬 출력값 중 가장 큰 값을 가지는 단어)을 선택해서 `<s>` 다음 단어 (아래 예시에선 Je)로 확정합니다.
    * `<s>` 을 넣고 예측해서 구한 다음 단어 (Je)을 RNN의 인풋으로 넣고, 다시 다음에 올 수 있는 타겟 단어 후보군을 softmax 행렬 형태로 예측하면 다음에 올 가장 그럴 듯한 단어를 선택해서 다음단어 (아래 예시에선 suis)로 확정합니다.
    * 문장의 끝을 의미하는 `</s>` 가 나올 때까지 반복합니다.
    * <img src="https://raw.githubusercontent.com/jsstar522/jsstar522.github.io/master/static/img/_posts/20200908/4.jpg" alt="distribution" style="display:block; width:700px; margin: 0 auto;"/>
* 인공신경망 vs 통계기반 번역, 뭐가 다를까요?
  * https://zdnet.co.kr/view/?no=20161223190944
* n-gram은 무엇일까요?
  * <https://jsstar522.github.io/deeplearning/2020/09/06/word-embedding.html>
* PageRank 알고리즘은 어떻게 작동하나요?
  * 구글 검색 엔진의 핵심이 된 알고리즘 (래리 페이지와 세이브리 게린의 논문)
  * 링크로 연결되어 있는 많은 페이지가 있을 때, 각 페이지의 페이지 랭크를 계산합니다. A 페이지의 페이지 랭크를 계산한다고 했을 때, A 페이지의 링크를 포함하고 있는 페이지 B, C 의 개수 (2개)와 페이지 B, C가 가르키고 있는 페이지의 총 개수, 페이지 랭크를 이용하여 계산합니다. 그렇게 계산된 값은 PR(A)로 계산되고 이 값은 다른 페이지 랭크를 계산할 때 또 사용됩니다. 또한 damping factor (DF)도 계산에 사용되는데, 이는 그 페이지에서 다른 페이지를 클릭할 확률을 의미합니다.
  * 값이 높을수록 중요한 페이지임을 의미합니다.
  * https://eyeballs.tistory.com/36
* dependency parsing이란 무엇인가요?
  * 문장의 문법적 구조를 파악하여 각 단어의 관계성을 찾는 방법입니다. 문장의 구조적인 모호성을 해결하고 이를 이용하여 다음 단어의 의미를 파악하기 위해 사용됩니다.
  * Constituency parsing은 트리구조를 이루고 구 단위로 잘라나가면서 단어의 문법적 종속성을 파악합니다.
  * Dependency parsing은 단어들의 관계에 집중합니다. 구 단위로 쪼개면서 만들어가는게 아닌 단어 관계로 쪼개면서 트리를 만들어가기 때문에 복잡도가 줄어듭니다. 단어와의 관계를 파악하기 때문에 더 자주 사용되는 것으로 알고 있습니다.
* word2vec의 원리는 무엇인가요? (그림에서 왼쪽 파라미터들을 임베딩으로 쓰는 이유는? 오른쪽 파라미터의 의미는? 남자와 여자가 가까울까? 남자와 자동차가 가까울까? 번역을 unsupervised로 할 수 있을까?)
  * <https://jsstar522.github.io/deeplearning/2020/09/06/word-embedding.html>

* LSA (잠재의미분석)은 무엇일까요?
  * 